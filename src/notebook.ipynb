{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c642a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import math\n",
    "import cv2\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05c7bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    train_dir  = '../data/hecto/train'\n",
    "    test_dir = '../data/hecto/test'\n",
    "    submission_csv    = '../data/hecto/sample_submission.csv'\n",
    "    test_meta      = '../data/hecto/test.csv'\n",
    "    name = \"efficientnet_b0.ra_in1k\"\n",
    "    epochs = 2\n",
    "    batch_size = 32\n",
    "    num_workers = 4             \n",
    "    seed = 42                       \n",
    "    optimizer = \"AdamW\"\n",
    "    lr = 2.0e-3                     \n",
    "    weight_decay = 1.0e-4\n",
    "    scheduler = \"CosineAnnealingLR\"\n",
    "    eta_min = 1.0e-6\n",
    "    device = \"cuda\"\n",
    "    n_fold = 4\n",
    "    selected_folds = [0,1,2,3]\n",
    "    T_max = 50\n",
    "    target_shape = [224, 224]\n",
    "    mixup_alpha = 0.5\n",
    "    in_channels = 3\n",
    "    pretrained = True\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "models_dir = Path(\"../models\")\n",
    "num_classes = pd.read_csv(\n",
    "   cfg.submission_csv\n",
    ").columns[1:].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce9a13d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24fb45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ 1) Pad ‚Üí Denoise helper ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def pad_denoise_mask(pil_img: Image.Image, patch_size: int, denoise_h: int = 10):\n",
    "    \"\"\"\n",
    "    Given a PIL image of arbitrary size (e.g., 553√ó402), pad its dimensions \n",
    "    up to the next multiple of patch_size, apply denoising, and return:\n",
    "      - img_masked: a NumPy array of shape (h_pad, w_pad, 3)\n",
    "      - (h_pad, w_pad): the padded height and width\n",
    "      - (orig_w, orig_h): the original width and height\n",
    "      - (x1, y1, x2, y2): coordinates of the valid (original) region\n",
    "        (always (0, 0, orig_w, orig_h))\n",
    "\n",
    "    Steps:\n",
    "      1. Convert PIL ‚Üí NumPy.\n",
    "      2. Compute pad so that both h_pad and w_pad are multiples of patch_size.\n",
    "      3. Pad with zeros on right/bottom.\n",
    "      4. Run OpenCV colored denoising (fastNlMeansDenoisingColored).\n",
    "      5. Apply a ‚Äúmask‚Äù that retains exactly the original image region \n",
    "         and zeroes everything in the padded margins.\n",
    "\n",
    "    Returns:\n",
    "        img_masked: np.ndarray of dtype uint8, shape (h_pad, w_pad, 3)\n",
    "        (h_pad, w_pad): padded dims\n",
    "        (orig_w, orig_h): original dims\n",
    "        (x1, y1, x2, y2): coordinates of the valid region\n",
    "    \"\"\"\n",
    "    img_np = np.array(pil_img)\n",
    "    orig_h, orig_w = img_np.shape[:2]\n",
    "    ps = patch_size\n",
    "\n",
    "    # (a) Compute padded dimensions\n",
    "    w_pad = math.ceil(orig_w / ps) * ps\n",
    "    h_pad = math.ceil(orig_h / ps) * ps\n",
    "    pad_right  = w_pad - orig_w\n",
    "    pad_bottom = h_pad - orig_h\n",
    "\n",
    "    # (b) Pad with zeros on right/bottom\n",
    "    img_padded = cv2.copyMakeBorder(\n",
    "        img_np,\n",
    "        top=0,\n",
    "        bottom=pad_bottom,\n",
    "        left=0,\n",
    "        right=pad_right,\n",
    "        borderType=cv2.BORDER_CONSTANT,\n",
    "        value=[0, 0, 0]\n",
    "    )\n",
    "\n",
    "    # (c) Denoise (OpenCV expects BGR)\n",
    "    bgr           = cv2.cvtColor(img_padded, cv2.COLOR_RGB2BGR)\n",
    "    denoised_bgr  = cv2.fastNlMeansDenoisingColored(\n",
    "                        bgr, None, denoise_h, denoise_h, 7, 21\n",
    "                    )\n",
    "    img_denoised  = cv2.cvtColor(denoised_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # (d) Build a mask that keeps only the original region\n",
    "    x1, y1, x2, y2 = 0, 0, orig_w, orig_h\n",
    "    mask2d = np.zeros((h_pad, w_pad), dtype=np.uint8)\n",
    "    mask2d[y1:y2, x1:x2] = 1\n",
    "    mask3c = np.stack([mask2d] * 3, axis=2)  # shape (h_pad, w_pad, 3)\n",
    "\n",
    "    img_masked = img_denoised * mask3c\n",
    "\n",
    "    return img_masked, (h_pad, w_pad), (orig_w, orig_h), (x1, y1, x2, y2)\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2) Stage 1 Dataset: one center‚Äêcrop per image ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class Stage1Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Stage 1: For each image, load it, resize so that the shorter side ‚â• target_size,\n",
    "    then take a single center‚Äêcrop of size `target_size` (e.g. 224√ó224). \n",
    "    __getitem__ returns (tensor, label_int).\n",
    "\n",
    "    Args:\n",
    "      image_paths: List[str] of full JPG/PNG paths.\n",
    "      labels:      List[int] of corresponding integer labels.\n",
    "      target_size: Tuple (tw, th), e.g. (224, 224).\n",
    "      transform:   torchvision transforms to apply after cropping\n",
    "                   (e.g., ToTensor + Normalize).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_paths: List[str],\n",
    "        labels: List[int],\n",
    "        target_size: Tuple[int, int],\n",
    "        transform=None\n",
    "    ):\n",
    "        assert len(image_paths) == len(labels)\n",
    "        self.paths       = image_paths\n",
    "        self.labels      = labels\n",
    "        self.target_size = target_size\n",
    "        self.transform   = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        path  = self.paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        pil   = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        # 1) Resize so that both dimensions ‚â• max(target_size)\n",
    "        tw, th = self.target_size\n",
    "        s = max(tw, th)\n",
    "        pil = pil.resize((s, s))\n",
    "\n",
    "        # 2) Center‚Äêcrop to (tw, th)\n",
    "        w, h = pil.size\n",
    "        x0 = (w - tw) // 2\n",
    "        y0 = (h - th) // 2\n",
    "        crop = pil.crop((x0, y0, x0 + tw, y0 + th))\n",
    "\n",
    "        # 3) Apply transforms (ToTensor + Normalize, etc.)\n",
    "        if self.transform:\n",
    "            crop = self.transform(crop)\n",
    "\n",
    "        return crop, label\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 3) Stage 2 Dataset: cache padded+denoised in RAM, sample random patches ‚îÄ‚îÄ‚îÄ\n",
    "class Stage2Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Stage 2: For each image, do pad+denoise exactly once (per DataLoader worker), cache \n",
    "    the result as a NumPy array, then repeatedly sample `num_patches_per_image` random \n",
    "    patches of shape (patch_size √ó patch_size). __getitem__ returns \n",
    "    (patch_tensor, one_hot_label).\n",
    "\n",
    "    Note:\n",
    "      - Each worker has its own `self.cache` dict, so no cross-worker locking is required.\n",
    "      - For an image of arbitrary original size (e.g. 553√ó402), pad_denoise_mask\n",
    "        will pad up to multiples of patch_size (e.g. 672√ó672 if patch_size = 224).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        items: List[Tuple[str, int]],    # List of (jpg_path, label_int)\n",
    "        patch_size: int = 224,\n",
    "        num_patches_per_image: int = 4,\n",
    "        denoise_h: int = 10,\n",
    "        transform=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.items       = items\n",
    "        self.patch_size  = patch_size\n",
    "        self.num_patches = num_patches_per_image\n",
    "        self.denoise_h   = denoise_h\n",
    "        self.transform   = transform\n",
    "\n",
    "        # Cache: jpg_path -> (denoised_np (h_pad, w_pad, 3), h_pad, w_pad)\n",
    "        self.cache: Dict[str, Tuple[np.ndarray, int, int]] = {}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # total # of patches across all images = len(items) * num_patches_per_image\n",
    "        return len(self.items) * self.num_patches\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        img_idx = idx // self.num_patches\n",
    "        jpg_path, label = self.items[img_idx]\n",
    "\n",
    "        # If not cached yet, load + pad+denoise + cache\n",
    "        if jpg_path not in self.cache:\n",
    "            pil = Image.open(jpg_path).convert(\"RGB\")\n",
    "            denoised_np, (h_pad, w_pad), (_, _), (_, _, _, _) = pad_denoise_mask(\n",
    "                pil,\n",
    "                self.patch_size,\n",
    "                denoise_h=self.denoise_h\n",
    "            )\n",
    "            self.cache[jpg_path] = (denoised_np, h_pad, w_pad)\n",
    "        else:\n",
    "            denoised_np, h_pad, w_pad = self.cache[jpg_path]\n",
    "\n",
    "        # Sample one random patch of size patch_size √ó patch_size\n",
    "        x0 = random.randint(0, w_pad - self.patch_size)\n",
    "        y0 = random.randint(0, h_pad - self.patch_size)\n",
    "        patch_np = denoised_np[y0 : y0 + self.patch_size, x0 : x0 + self.patch_size, :]\n",
    "\n",
    "        patch_pil = Image.fromarray(patch_np)\n",
    "        if self.transform:\n",
    "            patch_tensor = self.transform(patch_pil)\n",
    "        else:\n",
    "            patch_tensor = transforms.ToTensor()(patch_pil)\n",
    "\n",
    "        # One-hot label for BCEWithLogitsLoss\n",
    "        label_onehot = torch.zeros(num_classes, dtype=torch.float32)\n",
    "        label_onehot[label] = 1.0\n",
    "\n",
    "        return patch_tensor, label_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f5af70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(labels: List[int], num_classes: int) -> List[float]:\n",
    "    \"\"\"\n",
    "    Given a list of integer labels (0 to num_classes-1), compute per-sample weights \n",
    "    inversely proportional to each class‚Äôs frequency. Returns a list of weights aligned \n",
    "    with the input `labels` list.\n",
    "    \"\"\"\n",
    "    # Count how many examples belong to each class\n",
    "    counts = np.bincount(labels, minlength=num_classes)\n",
    "    # For any class with zero examples, set count to 1 to avoid division by zero\n",
    "    counts = np.where(counts == 0, 1, counts)\n",
    "    # Inverse frequency for each class\n",
    "    class_weights = 1.0 / counts  # array of length num_classes\n",
    "    # Assign each sample the weight corresponding to its label\n",
    "    sample_weights = [class_weights[label] for label in labels]\n",
    "    return sample_weights\n",
    "\n",
    "\n",
    "class FullImageLoader:\n",
    "    \"\"\"\n",
    "    Scans a training directory with subfolders per class and builds:\n",
    "      - self.paths:  List[str] of full image paths\n",
    "      - self.labels: List[int] of integer labels (index into `self.classes`)\n",
    "      - self.classes: sorted List[str] of class names (folder names)\n",
    "      - self.class_to_idx: Dict[str, int] mapping class name ‚Üí index\n",
    "      - self.idx_to_class: Dict[int, str] mapping index ‚Üí class name\n",
    "\n",
    "    Expects:\n",
    "        train_dir/\n",
    "            classA/\n",
    "                img1.jpg\n",
    "                img2.png\n",
    "                ...\n",
    "            classB/\n",
    "                img3.jpg\n",
    "                ...\n",
    "            ...\n",
    "    \"\"\"\n",
    "    def __init__(self, train_dir: str):\n",
    "        self.train_dir = Path(train_dir)\n",
    "        # List all immediate subdirectories (each represents one class), sorted\n",
    "        self.classes = sorted([d.name for d in self.train_dir.iterdir() if d.is_dir()])\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        self.idx_to_class = {idx: cls_name for cls_name, idx in self.class_to_idx.items()}\n",
    "\n",
    "        self.paths: List[str] = []\n",
    "        self.labels: List[int] = []\n",
    "\n",
    "        for cls_name in self.classes:\n",
    "            cls_idx = self.class_to_idx[cls_name]\n",
    "            cls_folder = self.train_dir / cls_name\n",
    "            # Iterate over files in this class folder in sorted order\n",
    "            for img_file in sorted(cls_folder.iterdir()):\n",
    "                if img_file.suffix.lower() not in {\".jpg\", \".jpeg\", \".png\"}:\n",
    "                    continue\n",
    "                self.paths.append(str(img_file))\n",
    "                self.labels.append(cls_idx)\n",
    "\n",
    "        # Sanity check\n",
    "        assert len(self.paths) == len(self.labels), \"Image-paths and labels lengths mismatch\"\n",
    "\n",
    "class PatchInferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    For inference on test images: extract a grid of possibly overlapping patches from each image.\n",
    "    __getitem__ returns (patch_batch, filename), where:\n",
    "      - patch_batch: torch.Tensor of shape [num_patches, 3, patch_size, patch_size]\n",
    "      - filename:    the base filename (e.g., \"abc.jpg\")\n",
    "\n",
    "    Args:\n",
    "        test_image_paths: List[str] of full paths to test images.\n",
    "        patch_size:       side length of each square patch (e.g. 255).\n",
    "        stride:           pixel step between adjacent patches (e.g. 240 for 15-pixel overlap).\n",
    "        transform:        torchvision transforms to apply to each patch \n",
    "                          (e.g. ToTensor + Normalize).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        test_image_paths: List[str],\n",
    "        patch_size: int = 255,\n",
    "        stride: int = 240,\n",
    "        transform: Optional[transforms.Compose] = None,\n",
    "    ):\n",
    "        self.test_paths = test_image_paths\n",
    "        self.patch_size = patch_size\n",
    "        self.stride     = stride\n",
    "        self.transform  = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.test_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, str]:\n",
    "        img_path = self.test_paths[idx]\n",
    "        pil_img  = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h     = pil_img.size  # e.g. 1024√ó1024 or arbitrary\n",
    "\n",
    "        patches: List[torch.Tensor] = []\n",
    "        ps = self.patch_size\n",
    "        st = self.stride\n",
    "\n",
    "        # 1) Slide window top-to-bottom, left-to-right\n",
    "        for y in range(0, h - ps + 1, st):\n",
    "            for x in range(0, w - ps + 1, st):\n",
    "                patch = pil_img.crop((x, y, x + ps, y + ps))\n",
    "                if self.transform:\n",
    "                    patch = self.transform(patch)\n",
    "                patches.append(patch)\n",
    "\n",
    "        # 2) Handle bottom row if not exactly divisible by stride\n",
    "        if (h - ps) % st != 0:\n",
    "            y0 = h - ps\n",
    "            for x in range(0, w - ps + 1, st):\n",
    "                patch = pil_img.crop((x, y0, x + ps, y0 + ps))\n",
    "                if self.transform:\n",
    "                    patch = self.transform(patch)\n",
    "                patches.append(patch)\n",
    "\n",
    "        # 3) Handle right column if not exactly divisible by stride\n",
    "        if (w - ps) % st != 0:\n",
    "            x0 = w - ps\n",
    "            for y in range(0, h - ps + 1, st):\n",
    "                patch = pil_img.crop((x0, y, x0 + ps, y + ps))\n",
    "                if self.transform:\n",
    "                    patch = self.transform(patch)\n",
    "                patches.append(patch)\n",
    "\n",
    "        # 4) Handle bottom-right corner if both dims had remainders\n",
    "        if (h - ps) % st != 0 and (w - ps) % st != 0:\n",
    "            patch = pil_img.crop((w - ps, h - ps, w, h))\n",
    "            if self.transform:\n",
    "                patch = self.transform(patch)\n",
    "            patches.append(patch)\n",
    "\n",
    "        # Stack all patches into a single tensor [num_patches, 3, ps, ps]\n",
    "        patch_batch = torch.stack(patches, dim=0)\n",
    "        filename    = os.path.basename(img_path)\n",
    "        return patch_batch, filename\n",
    "\n",
    "\n",
    "def calculate_log_loss(targets: np.ndarray, outputs: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute multi-class log loss (cross-entropy) given:\n",
    "      - targets: np.ndarray of shape [N], each an integer in [0..C-1]\n",
    "      - outputs: np.ndarray of shape [N, C], each row a predicted probability vector (summing to 1)\n",
    "\n",
    "    Steps:\n",
    "      1. Clip outputs into [eps, 1-eps]\n",
    "      2. Re-normalize so each row sums to 1\n",
    "      3. Use sklearn‚Äôs log_loss with labels=[0..C-1]\n",
    "\n",
    "    Returns:\n",
    "        float: the overall log-loss.\n",
    "    \"\"\"\n",
    "    num_classes = outputs.shape[1]\n",
    "    eps = 1e-15\n",
    "\n",
    "    clipped = np.clip(outputs, eps, 1 - eps)\n",
    "    clipped = clipped / clipped.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return float(log_loss(targets, clipped, labels=list(range(num_classes))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "482cb4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Fix random seeds and set device\n",
    "# ----------------------------------------------------------------------------\n",
    "seed = cfg.seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(cfg.device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Model definition with soft‚Äêlabel BCE and optional MixUp\n",
    "# ----------------------------------------------------------------------------\n",
    "class Hecto_EFFICIENTNET(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        in_chans: int,\n",
    "        num_classes: int,\n",
    "        pretrained: bool = True,\n",
    "        mixup_alpha: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes   = num_classes\n",
    "        self.mixup_alpha   = mixup_alpha\n",
    "        self.mixup_enabled = mixup_alpha > 0.0\n",
    "\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            in_chans=in_chans,\n",
    "            num_classes=num_classes,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, targets: torch.Tensor = None):\n",
    "        # If training and mixup is enabled, perform MixUp\n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "            idx = torch.randperm(x.size(0), device=x.device)\n",
    "            x_mixed = lam * x + (1 - lam) * x[idx]\n",
    "            y_a, y_b = targets, targets[idx]\n",
    "            logits = self.backbone(x_mixed)\n",
    "            loss = (\n",
    "                lam * F.binary_cross_entropy_with_logits(logits, y_a)\n",
    "                + (1 - lam) * F.binary_cross_entropy_with_logits(logits, y_b)\n",
    "            )\n",
    "            return logits, loss\n",
    "\n",
    "        # Otherwise, just forward normally\n",
    "        logits = self.backbone(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Optimizer, scheduler, criterion factories\n",
    "# ----------------------------------------------------------------------------\n",
    "def get_optimizer(model: nn.Module, cfg: CFG) -> optim.Optimizer:\n",
    "    opt = cfg.optimizer\n",
    "    lr  = cfg.lr\n",
    "    wd  = cfg.weight_decay\n",
    "\n",
    "    if opt == \"Adam\":\n",
    "        return optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    if opt == \"AdamW\":\n",
    "        return optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    if opt == \"SGD\":\n",
    "        return optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "\n",
    "    raise ValueError(f\"Unsupported optimizer: {opt}\")\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer: optim.Optimizer, cfg: CFG):\n",
    "    sch = cfg.scheduler\n",
    "\n",
    "    if sch == \"CosineAnnealingLR\":\n",
    "        return lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=cfg.T_max,\n",
    "            eta_min=cfg.eta_min,\n",
    "        )\n",
    "    if sch == \"ReduceLROnPlateau\":\n",
    "        return lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=cfg.eta_min,\n",
    "            verbose=True,\n",
    "        )\n",
    "    if sch == \"StepLR\":\n",
    "        return lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=max(1, cfg.epochs // 3),\n",
    "            gamma=0.5,\n",
    "        )\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_criterion(cfg: CFG):\n",
    "    return nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Full‚Äêimage validation dataset (patchify & average)\n",
    "# ----------------------------------------------------------------------------\n",
    "class FullValDataset(Dataset):\n",
    "    \"\"\"\n",
    "    For validation: given a list of (image_path, label), run pad‚Üídenoise‚Üímask,\n",
    "    then extract all non‚Äêoverlapping patches of size `patch_size` and stack them.\n",
    "    __getitem__ returns (patch_tensor, label_int), where patch_tensor has shape\n",
    "    [num_patches, 3, patch_size, patch_size].\n",
    "    \"\"\"\n",
    "    def __init__(self, val_items: List[Tuple[str, int]], patch_size: int, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            val_items:  List of (file_path, label_int).\n",
    "            patch_size: side length of square patch (e.g., 224).\n",
    "            transform:  torchvision transforms to apply to each patch.\n",
    "        \"\"\"\n",
    "        self.items      = val_items\n",
    "        self.patch_size = patch_size\n",
    "        self.transform  = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        path, label = self.items[idx]\n",
    "        pil = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        # Pad and denoise, then apply full‚Äêimage mask\n",
    "        img_masked_np, (h_pad, w_pad), (_, _), (_, _, _, _) = pad_denoise_mask(\n",
    "            pil, self.patch_size, denoise_h=10\n",
    "        )\n",
    "\n",
    "        # Extract all non‚Äêoverlapping patches\n",
    "        patches = []\n",
    "        for top in range(0, h_pad, self.patch_size):\n",
    "            for left in range(0, w_pad, self.patch_size):\n",
    "                patch_np = img_masked_np[top : top + self.patch_size, left : left + self.patch_size]\n",
    "                p = Image.fromarray(patch_np)\n",
    "                if self.transform:\n",
    "                    p = self.transform(p)\n",
    "                patches.append(p)\n",
    "\n",
    "        patch_tensor = torch.stack(patches, dim=0)  # [num_patches, 3, patch_size, patch_size]\n",
    "        return patch_tensor, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e9f6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    # ‚îÄ‚îÄ‚îÄ Prepare full loader and precompute all patches ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    full_loader = FullImageLoader(cfg.train_dir)\n",
    "    labels_arr  = np.array(full_loader.labels)\n",
    "\n",
    "    patch_size = cfg.target_shape[0]\n",
    "    precomputed_patches: Dict[str, List[np.ndarray]] = {}\n",
    "    print(\"Precomputing padded+denoised patches for every training image ...\")\n",
    "    for img_path in full_loader.paths:\n",
    "        pil = Image.open(img_path).convert(\"RGB\")\n",
    "        denoised_np, (h_pad, w_pad), (_, _), (_, _, _, _) = pad_denoise_mask(\n",
    "            pil, patch_size, denoise_h=10\n",
    "        )\n",
    "        patches = []\n",
    "        for top in range(0, h_pad, patch_size):\n",
    "            for left in range(0, w_pad, patch_size):\n",
    "                patch_np = denoised_np[top : top + patch_size, left : left + patch_size, :]\n",
    "                patches.append(patch_np)\n",
    "        precomputed_patches[img_path] = patches\n",
    "    total_patches = sum(len(v) for v in precomputed_patches.values())\n",
    "    print(f\"‚Üí Done. {len(full_loader.paths)} images ‚Üí {total_patches} patches\")\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ Cross‚Äêvalidation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    skf = StratifiedKFold(n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n",
    "    best_scores = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(np.zeros(len(labels_arr)), labels_arr)):\n",
    "        if fold not in cfg.selected_folds:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n===== Fold {fold} =====\")\n",
    "        ckpt_path = models_dir / f\"{cfg.name}_fold{fold}_best.pth\"\n",
    "\n",
    "        # ‚îÄ‚îÄ‚îÄ Instantiate model, optimizer, scheduler, criterion ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        model     = Hecto_EFFICIENTNET(\n",
    "                        cfg.name,\n",
    "                        cfg.in_channels,\n",
    "                        num_classes,\n",
    "                        pretrained=cfg.pretrained,\n",
    "                        mixup_alpha=cfg.mixup_alpha\n",
    "                    ).to(device)\n",
    "        optimizer = get_optimizer(model, cfg)\n",
    "        scheduler = get_scheduler(optimizer, cfg)\n",
    "        criterion = get_criterion(cfg)\n",
    "\n",
    "        start_epoch = 0\n",
    "        if ckpt_path.exists() and not cfg.pretrained:\n",
    "            state = torch.load(ckpt_path, map_location=device)\n",
    "            model.load_state_dict(state[\"model_state_dict\"])\n",
    "            optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "            if state.get(\"scheduler_state_dict\") is not None and scheduler:\n",
    "                scheduler.load_state_dict(state[\"scheduler_state_dict\"])\n",
    "            start_epoch = state.get(\"epoch\", 0)\n",
    "            print(f\"Resumed fold {fold} at epoch {start_epoch}\")\n",
    "\n",
    "        # ‚îÄ‚îÄ‚îÄ Build train_items & val_items ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        train_items = [(full_loader.paths[i], full_loader.labels[i]) for i in tr_idx]\n",
    "        val_items   = [(full_loader.paths[i], full_loader.labels[i]) for i in va_idx]\n",
    "\n",
    "        # ‚îÄ‚îÄ‚îÄ STAGE 1: One center‚Äêcrop per image ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        stage1_ds = Stage1Dataset(\n",
    "            image_paths = [p for (p, _) in train_items],\n",
    "            labels      = [lbl for (_, lbl) in train_items],\n",
    "            target_size = tuple(cfg.target_shape),\n",
    "            transform   = transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                                                   std =[0.229,0.224,0.225]),\n",
    "                          ])\n",
    "        )\n",
    "        stage1_loader = DataLoader(\n",
    "            stage1_ds,\n",
    "            batch_size  = cfg.batch_size,\n",
    "            shuffle     = True,\n",
    "            num_workers = cfg.num_workers,\n",
    "            pin_memory  = True,\n",
    "        )\n",
    "\n",
    "        N1 = 1\n",
    "        best_logloss_stage1 = float(\"inf\")\n",
    "\n",
    "        for e1 in range(N1):\n",
    "            print(f\"\\n>>> Fold {fold} | Stage 1 Epoch {e1+1}/{N1} <<<\")\n",
    "            model.train()\n",
    "            total_loss1 = 0.0\n",
    "            all_targets1, all_probs1 = [], []\n",
    "\n",
    "            for imgs, labels_int in tqdm(stage1_loader, desc=\"Stage1 Training\"):\n",
    "                imgs     = imgs.to(device)\n",
    "                labels_i = labels_int.to(device)\n",
    "                one_hot  = F.one_hot(labels_i, num_classes).float()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                out = model(imgs, one_hot)\n",
    "                if isinstance(out, tuple):\n",
    "                    logits1, loss1 = out\n",
    "                else:\n",
    "                    logits1 = out\n",
    "                    loss1   = criterion(logits1, one_hot)\n",
    "\n",
    "                loss1.backward()\n",
    "                optimizer.step()\n",
    "                if isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                    scheduler.step()\n",
    "\n",
    "                total_loss1 += loss1.item() * imgs.size(0)\n",
    "                probs1 = torch.sigmoid(logits1).detach().cpu().numpy()\n",
    "                all_probs1.append(probs1)\n",
    "                all_targets1.append(labels_i.cpu().numpy())\n",
    "\n",
    "            avg_loss1    = total_loss1 / len(stage1_loader.dataset)\n",
    "            all_probs1   = np.vstack(all_probs1)\n",
    "            all_targets1 = np.concatenate(all_targets1)\n",
    "            eps    = 1e-15\n",
    "            clipped1 = np.clip(all_probs1, eps, 1 - eps)\n",
    "            clipped1 = clipped1 / clipped1.sum(axis=1, keepdims=True)\n",
    "            val_logloss1 = log_loss(all_targets1, clipped1, labels=list(range(num_classes)))\n",
    "            print(f\"    Stage1: Train Loss {avg_loss1:.4f} | LogLoss {val_logloss1:.4f}\")\n",
    "\n",
    "            if val_logloss1 < best_logloss_stage1:\n",
    "                best_logloss_stage1 = val_logloss1\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    models_dir / f\"{cfg.name}_fold{fold}_stage1.pth\"\n",
    "                )\n",
    "\n",
    "            if scheduler and not isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(val_logloss1)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        # ‚îÄ‚îÄ‚îÄ STAGE 2: use precomputed patches ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        stage2_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std =[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        stage2_ds = Stage2Dataset(\n",
    "            items       = train_items,\n",
    "            precomputed = precomputed_patches,\n",
    "            transform   = stage2_transform,\n",
    "        )\n",
    "\n",
    "        patch_labels = [lbl for (_, _, lbl) in stage2_ds.index_map]\n",
    "        patch_weights = compute_class_weights(patch_labels, num_classes)\n",
    "\n",
    "        sampler2 = WeightedRandomSampler(\n",
    "            weights     = patch_weights,\n",
    "            num_samples = len(patch_weights),\n",
    "            replacement = False\n",
    "        )\n",
    "        stage2_loader = DataLoader(\n",
    "            stage2_ds,\n",
    "            batch_size  = cfg.batch_size,\n",
    "            sampler     = sampler2,\n",
    "            num_workers = cfg.num_workers,\n",
    "            pin_memory  = True,\n",
    "        )\n",
    "\n",
    "        N2 = cfg.epochs - N1\n",
    "        best_logloss_stage2 = float(\"inf\")\n",
    "\n",
    "        for epoch2 in range(N2):\n",
    "            print(f\"\\n>>> Fold {fold} | Stage 2 Epoch {epoch2+1}/{N2} <<<\")\n",
    "            model.train()\n",
    "            total_loss2 = 0.0\n",
    "            all_targets2, all_probs2 = [], []\n",
    "\n",
    "            for patches, labels_onehot in tqdm(stage2_loader, desc=\"Stage2 Training\"):\n",
    "                patches  = patches.to(device)\n",
    "                labels_oh = labels_onehot.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                out2 = model(patches, labels_oh)\n",
    "                if isinstance(out2, tuple):\n",
    "                    logits2, loss2 = out2\n",
    "                else:\n",
    "                    logits2 = out2\n",
    "                    loss2   = criterion(logits2, labels_oh)\n",
    "\n",
    "                loss2.backward()\n",
    "                optimizer.step()\n",
    "                if isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                    scheduler.step()\n",
    "\n",
    "                total_loss2 += loss2.item() * patches.size(0)\n",
    "                probs2 = torch.sigmoid(logits2).detach().cpu().numpy()\n",
    "                all_probs2.append(probs2)\n",
    "                labels_int2 = torch.argmax(labels_oh, dim=1).cpu().numpy()\n",
    "                all_targets2.append(labels_int2)\n",
    "\n",
    "            avg_loss2   = total_loss2 / len(stage2_loader.dataset)\n",
    "            all_probs2  = np.vstack(all_probs2)\n",
    "            all_targets2 = np.concatenate(all_targets2)\n",
    "\n",
    "            num_images_fold = len(train_items)\n",
    "            all_probs2 = all_probs2.reshape(num_images_fold, -1, num_classes)\n",
    "            avg_probs_img = all_probs2.mean(axis=1)\n",
    "\n",
    "            eps     = 1e-15\n",
    "            clipped2 = np.clip(avg_probs_img, eps, 1 - eps)\n",
    "            clipped2 = clipped2 / clipped2.sum(axis=1, keepdims=True)\n",
    "            labels_img = np.array([lbl for (_, lbl) in train_items])\n",
    "            val_logloss2 = log_loss(labels_img, clipped2, labels=list(range(num_classes)))\n",
    "            print(f\"    Stage2: Train Loss {avg_loss2:.4f} | Image‚Äêlevel LogLoss {val_logloss2:.4f}\")\n",
    "\n",
    "            if val_logloss2 < best_logloss_stage2:\n",
    "                best_logloss_stage2 = val_logloss2\n",
    "                torch.save({\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n",
    "                    \"epoch\": N1 + epoch2 + 1,\n",
    "                    \"val_logloss\": val_logloss2\n",
    "                }, ckpt_path)\n",
    "                print(f\"      ‚Üí Saved new best Stage2 checkpoint (LogLoss: {best_logloss_stage2:.4f})\")\n",
    "\n",
    "            if scheduler and not isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(val_logloss2)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "        best_scores.append(best_logloss_stage2)\n",
    "\n",
    "        # ‚îÄ‚îÄ‚îÄ Fold‚Äêlevel validation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        val_ds = FullValDataset(\n",
    "            val_items  = val_items,\n",
    "            patch_size = cfg.target_shape[0],\n",
    "            transform  = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                                                 std =[0.229,0.224,0.225]),\n",
    "                        ])\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_ds,\n",
    "            batch_size   = 1,\n",
    "            shuffle      = False,\n",
    "            num_workers  = cfg.num_workers,\n",
    "            pin_memory   = True\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        fold_preds, fold_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for patch_batch, label_int in tqdm(val_loader, desc=\"Fold Validation\"):\n",
    "                patches = patch_batch.squeeze(0)\n",
    "                all_logits = []\n",
    "                for i in range(0, patches.size(0), cfg.batch_size):\n",
    "                    chunk = patches[i : i + cfg.batch_size].to(device)\n",
    "                    logits_chunk = model(chunk)\n",
    "                    all_logits.append(logits_chunk.cpu())\n",
    "                all_logits = torch.cat(all_logits, dim=0)\n",
    "                avg_probs = torch.sigmoid(all_logits).mean(dim=0)\n",
    "                fold_preds.append(avg_probs.numpy())\n",
    "                fold_labels.append(label_int.item())\n",
    "\n",
    "        fold_preds   = np.vstack(fold_preds)\n",
    "        fold_labels  = np.array(fold_labels)\n",
    "        eps          = 1e-15\n",
    "        clipped_val  = np.clip(fold_preds, eps, 1 - eps)\n",
    "        clipped_val  = clipped_val / clipped_val.sum(axis=1, keepdims=True)\n",
    "        fold_logloss = log_loss(fold_labels, clipped_val, labels=list(range(num_classes)))\n",
    "        print(f\"Fold {fold} final LogLoss: {fold_logloss:.4f}\")\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ Cross‚Äêvalidation summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    print(\"\\n===== CV Results =====\")\n",
    "    for f, score in zip(cfg.selected_folds, best_scores):\n",
    "        print(f\"Fold {f}: {score:.4f}\")\n",
    "    print(f\"Mean LogLoss: {np.mean(best_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4363c5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ YOLO11n successfully loaded via `YOLO(...)` interface\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.0ms preprocess, 2.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "üöÄ CUDA kernels warmed up with a dummy forward‚Äêpass\n",
      "\n",
      "Starting precompute for b0 (224√ó224), two stages\n",
      "\n",
      "=== Precomputing variant b0 (patch_size = 224) ===\n",
      "  [b0] Image 1/33137 ‚Äî elapsed 0.0s\n",
      "\n",
      "0: 352x640 1 car, 20.4ms\n",
      "Speed: 6.3ms preprocess, 20.4ms inference, 88.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 512x640 1 car, 18.9ms\n",
      "Speed: 0.9ms preprocess, 18.9ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 320x640 1 car, 16.0ms\n",
      "Speed: 0.6ms preprocess, 16.0ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 car, 2.7ms\n",
      "Speed: 0.6ms preprocess, 2.7ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 512x640 1 car, 2.9ms\n",
      "Speed: 0.8ms preprocess, 2.9ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 544x640 1 car, 15.9ms\n",
      "Speed: 0.8ms preprocess, 15.9ms inference, 0.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 320x640 1 car, 2.8ms\n",
      "Speed: 0.6ms preprocess, 2.8ms inference, 0.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 car, 2.6ms\n",
      "Speed: 0.6ms preprocess, 2.6ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 480x640 1 car, 16.0ms\n",
      "Speed: 0.8ms preprocess, 16.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 320x640 1 car, 2.9ms\n",
      "Speed: 0.6ms preprocess, 2.9ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 448x640 1 car, 15.9ms\n",
      "Speed: 0.7ms preprocess, 15.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 car, 2.8ms\n",
      "Speed: 0.7ms preprocess, 2.8ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 320x640 1 car, 2.8ms\n",
      "Speed: 0.6ms preprocess, 2.8ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 car, 2.6ms\n",
      "Speed: 0.6ms preprocess, 2.6ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 512x640 1 car, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 544x640 1 car, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 384x640 1 car, 15.9ms\n",
      "Speed: 0.7ms preprocess, 15.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 car, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 352x640 1 car, 2.8ms\n",
      "Speed: 0.6ms preprocess, 2.8ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 512x640 1 car, 2.8ms\n",
      "Speed: 0.7ms preprocess, 2.8ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 320x640 1 car, 2.7ms\n",
      "Speed: 0.6ms preprocess, 2.7ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 352x640 1 car, 2.8ms\n",
      "Speed: 0.6ms preprocess, 2.8ms inference, 0.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 480x640 1 car, 1 truck, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 car, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 1 car, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 1 car, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 320x640 1 car, 2.8ms\n",
      "Speed: 0.6ms preprocess, 2.8ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 384x640 2 cars, 2.8ms\n",
      "Speed: 0.7ms preprocess, 2.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m arr_s1         \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(resized_masked, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m     92\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mstr\u001b[39m(stage1_base \u001b[38;5;241m/\u001b[39m cls_name \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m), arr_s1)\n\u001b[0;32m---> 94\u001b[0m denoised_np, (h_pad, w_pad), (_, _), (_, _, _, _) \u001b[38;5;241m=\u001b[39m pad_denoise_mask(\n\u001b[1;32m     95\u001b[0m     masked, patch_size, denoise_h\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     97\u001b[0m patches_m \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m top \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, h_pad, patch_size):\n",
      "Cell \u001b[0;32mIn[28], line 49\u001b[0m, in \u001b[0;36mpad_denoise_mask\u001b[0;34m(pil_img, patch_size, denoise_h)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# (c) Denoise (OpenCV expects BGR)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m bgr           \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img_padded, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR)\n\u001b[0;32m---> 49\u001b[0m denoised_bgr  \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mfastNlMeansDenoisingColored(\n\u001b[1;32m     50\u001b[0m                     bgr, \u001b[38;5;28;01mNone\u001b[39;00m, denoise_h, denoise_h, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m21\u001b[39m\n\u001b[1;32m     51\u001b[0m                 )\n\u001b[1;32m     52\u001b[0m img_denoised  \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(denoised_bgr, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# (d) Build a mask that keeps only the original region\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ STEP 0: Load YOLO model\n",
    "yolo_weights_path = \"../models/yolo11n.pt\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "try:\n",
    "    model = YOLO(yolo_weights_path)\n",
    "    model.to(device).eval()\n",
    "    print(\"‚úÖ YOLO11n successfully loaded via `YOLO(...)` interface\\n\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"‚ùå Could not load YOLO11n weights:\\n  {e}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ STEP 1.1: Warm‚Äêup CUDA (avoids a large one‚Äêtime allocation crash)\n",
    "with torch.no_grad():\n",
    "    dummy = torch.zeros((1, 3, 640, 640), device=device, dtype=torch.float32)\n",
    "    _ = model(dummy)[0]\n",
    "del dummy\n",
    "print(\"üöÄ CUDA kernels warmed up with a dummy forward‚Äêpass\\n\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ STEP 2: Set up FullImageLoader, thresholds, and parameters\n",
    "full_loader   = FullImageLoader(cfg.train_dir)\n",
    "conf_thresh   = 0.25\n",
    "iou_thresh    = 0.45\n",
    "img_size_inf  = 640   # YOLO inference size (letterbox to 640√ó640)\n",
    "patch_size    = 224   # for b0 variant\n",
    "\n",
    "print(\"Starting precompute for b0 (224√ó224), two stages\\n\")\n",
    "\n",
    "variant      = \"b0\"\n",
    "start_all    = time.time()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Prepare output folders under /data/hecto/precomputed_b0\n",
    "out_base     = Path(\"/data\") / \"hecto\" / f\"precomputed_{variant}\"\n",
    "stage1_base  = out_base / \"stage1_masked_resized\"\n",
    "stage2_base  = out_base / \"stage2_masked_patchify\"\n",
    "stage1_base.mkdir(parents=True, exist_ok=True)\n",
    "stage2_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "total_images = len(full_loader.paths)\n",
    "print(f\"=== Precomputing variant {variant} (patch_size = {patch_size}) ===\")\n",
    "\n",
    "for idx, img_path in enumerate(full_loader.paths, start=1):\n",
    "    if idx % 100 == 1:\n",
    "        elapsed = time.time() - start_all\n",
    "        print(f\"  [{variant}] Image {idx}/{total_images} ‚Äî elapsed {elapsed:.1f}s\")\n",
    "\n",
    "    rel      = Path(img_path).relative_to(cfg.train_dir)\n",
    "    cls_name = rel.parent.name\n",
    "    stem     = rel.stem\n",
    "\n",
    "    (stage1_base / cls_name).mkdir(exist_ok=True, parents=True)\n",
    "    (stage2_base / cls_name).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    pil = Image.open(img_path).convert(\"RGB\")\n",
    "    img0 = np.array(pil)\n",
    "\n",
    "    results = model(pil, imgsz=img_size_inf)\n",
    "    dets    = results[0].boxes\n",
    "\n",
    "    boxes_xyxy = dets.xyxy.cpu().numpy()\n",
    "    confs      = dets.conf.cpu().numpy()\n",
    "    cls_idxs   = dets.cls.cpu().numpy()\n",
    "\n",
    "    car_mask = (cls_idxs == 2) & (confs >= conf_thresh)\n",
    "    if car_mask.sum() == 0:\n",
    "        masked = pil\n",
    "    else:\n",
    "        car_boxes = boxes_xyxy[car_mask]\n",
    "        areas     = (car_boxes[:, 2] - car_boxes[:, 0]) * (car_boxes[:, 3] - car_boxes[:, 1])\n",
    "        best_i    = int(np.argmax(areas))\n",
    "        x1, y1, x2, y2 = car_boxes[best_i].astype(int)\n",
    "\n",
    "        H, W, _ = img0.shape\n",
    "        x1 = max(0, min(x1, W - 1))\n",
    "        y1 = max(0, min(y1, H - 1))\n",
    "        x2 = max(0, min(x2, W))\n",
    "        y2 = max(0, min(y2, H))\n",
    "\n",
    "        masked   = Image.new(\"RGB\", pil.size, (0, 0, 0))\n",
    "        car_crop = pil.crop((x1, y1, x2, y2))\n",
    "        masked.paste(car_crop, (x1, y1))\n",
    "\n",
    "    resized_masked = masked.resize((patch_size, patch_size))\n",
    "    arr_s1         = np.array(resized_masked, dtype=np.uint8)\n",
    "    np.save(str(stage1_base / cls_name / f\"{stem}.npy\"), arr_s1)\n",
    "\n",
    "    denoised_np, (h_pad, w_pad), (_, _), (_, _, _, _) = pad_denoise_mask(\n",
    "        masked, patch_size, denoise_h=10\n",
    "    )\n",
    "    patches_m = []\n",
    "    for top in range(0, h_pad, patch_size):\n",
    "        for left in range(0, w_pad, patch_size):\n",
    "            tile = denoised_np[top : top + patch_size, left : left + patch_size, :]\n",
    "            patches_m.append(tile)\n",
    "    arr_s2 = np.stack(patches_m, axis=0).astype(np.uint8)\n",
    "    np.save(str(stage2_base / cls_name / f\"{stem}.npy\"), arr_s2)\n",
    "\n",
    "elapsed_total = time.time() - start_all\n",
    "print(f\"\\n‚Üí Finished variant {variant} in {elapsed_total:.1f}s, saved under {out_base}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dae9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/minkeymouse/kaggle_projects/hecto/src'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hecto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
